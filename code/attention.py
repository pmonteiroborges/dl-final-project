import math
from matplotlib import use
import numpy as np
import tensorflow as tf

class Attention(tf.keras.layers.Layer):
    def __init__(self, input_size, output_size, use_mask=False):
        super(Attention, self).__init__()

        self.use_mask = use_mask
     
        # used to compute the score in the attention calculation
        self.W = self.add_weight(shape=(input_size, output_size), trainable=True)

    def score(self, decoder_hidden_state, hidden_state):
        # TODO: might need to use tf.matmul or tf.tensordot somewhere instead
        return tf.transpose(decoder_hidden_state) @ self.W @ hidden_state

    def call(self, curr_decoder_hidden_state, encoder_hidden_states):
        '''
        Calculates the context vector.

        :param curr_decoder_hidden_state: the current hidden state of the decoder with shape (batch_size, units)
        :param encoder_hidden_states: all the hidden states generated by the encoder with shape (batch_size, timesteps, units)

        :return context: the context vector 
        '''
        scores = self.score(curr_decoder_hidden_state, encoder_hidden_states)
        softmaxed = tf.nn.softmax(scores)
        scaled = softmaxed * encoder_hidden_states

        context = tf.reduce_sum(scaled)

        return context




    